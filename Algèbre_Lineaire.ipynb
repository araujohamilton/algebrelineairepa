{"cells":[{"cell_type":"markdown","id":"7d9bc1fd","metadata":{"origin_pos":1,"id":"7d9bc1fd"},"source":["# Linear Algebra\n","\n","\n","By now, we can load datasets into tensors\n","and manipulate these tensors \n","with basic mathematical operations.\n","To start building sophisticated models,\n","we will also need a few tools from linear algebra. \n","This section offers a gentle introduction \n","to the most essential concepts,\n","starting from scalar arithmetic\n","and ramping up to matrix multiplication.\n","\n","\n","\n","## Scalars\n","\n","\n","Most everyday mathematics\n","consists of manipulating \n","numbers one at a time.\n","Formally, we call these values *scalars*.\n","For example, the temperature in Palo Alto \n","is a balmy $72$ degrees Fahrenheit.\n","If you wanted to convert the temperature to Celsius\n","you would evaluate the expression \n","$c = \\frac{5}{9}(f - 32)$, setting $f$ to $72$.\n","In this equation, the values \n","$5$, $9$, and $32$ are scalars.\n","The variables $c$ and $f$ \n","represent unknown scalars.\n","\n","We denote scalars\n","by ordinary lower-cased letters \n","(e.g., $x$, $y$, and $z$)\n","and the space of all (continuous) \n","*real-valued* scalars by $\\mathbb{R}$.\n","For expedience, we will skip past\n","rigorous definitions of *spaces*.\n","Just remember that the expression $x \\in \\mathbb{R}$\n","is a formal way to say that $x$ is a real-valued scalar.\n","The symbol $\\in$ (pronounced \"in\")\n","denotes membership in a set.\n","For example, $x, y \\in \\{0, 1\\}$\n","indicates that $x$ and $y$ are variables\n","that can only take values $0$ or $1$.\n","\n","(**Scalars are implemented as tensors \n","that contain only one element.**)\n","Below, we assign two scalars\n","and perform the familiar addition, multiplication,\n","division, and exponentiation operations.\n"]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"Au_C3rLhtVuK","executionInfo":{"status":"ok","timestamp":1668005601781,"user_tz":-60,"elapsed":6,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}}},"id":"Au_C3rLhtVuK","execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"id":"d60b66be","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:30.062491Z","iopub.status.busy":"2022-09-07T22:46:30.061682Z","iopub.status.idle":"2022-09-07T22:46:31.458870Z","shell.execute_reply":"2022-09-07T22:46:31.458010Z"},"origin_pos":2,"tab":["mxnet"],"id":"d60b66be","executionInfo":{"status":"ok","timestamp":1668005601782,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}}},"outputs":[],"source":["x = np.array(3.0)\n","y = np.array(2.0)"]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yjz5sG_MwOF3","executionInfo":{"status":"ok","timestamp":1668005601782,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"36378548-6a34-44f1-eaa6-50139c51fdc2"},"id":"Yjz5sG_MwOF3","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(3.)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x + y, x * y, x / y, x ** y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8h2FP6n3wPc3","executionInfo":{"status":"ok","timestamp":1668005602173,"user_tz":-60,"elapsed":396,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"605a363e-0e87-4b64-8600-840b5dbe7f1a"},"id":"8h2FP6n3wPc3","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5.0, 6.0, 1.5, 9.0)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","id":"871dea98","metadata":{"origin_pos":5,"id":"871dea98"},"source":["## Vectors\n","\n","For our purposes, [**you can think of vectors\n","as fixed-length arrays of scalars.**]\n","As with their code counterparts,\n","we call these values the *elements* of the vector\n","(synonyms include *entries* and *components*).\n","When vectors represent examples from real-world datasets,\n","their values hold some real-world significance.\n","For example, if we were training a model to predict\n","the risk of a loan defaulting,\n","we might associate each applicant with a vector\n","whose components correspond to quantities\n","like their income, length of employment, \n","or number of previous defaults.\n","If we were studying heart attack risk,\n","each vector might represent a patient\n","and its components might correspond to\n","their most recent vital signs, cholesterol levels, \n","minutes of exercise per day, etc.\n","We denote vectors by bold lowercase letters, \n","(e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$).\n","\n","Vectors are implemented as $1^{\\mathrm{st}}$-order tensors.\n","In general, such tensors can have arbitrary lengths,\n","subject to memory limitations. Caution: in Python, like in most programming languages, vector indices start at $0$, also known as *zero-based indexing*, whereas in linear algebra subscripts begin at $1$ (one-based indexing).\n"]},{"cell_type":"code","execution_count":5,"id":"44d17347","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.462843Z","iopub.status.busy":"2022-09-07T22:46:31.462169Z","iopub.status.idle":"2022-09-07T22:46:31.468529Z","shell.execute_reply":"2022-09-07T22:46:31.467798Z"},"origin_pos":6,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"44d17347","executionInfo":{"status":"ok","timestamp":1668005602174,"user_tz":-60,"elapsed":33,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"924f4c65-aa4b-42ff-a3cb-01fe71541ee6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2])"]},"metadata":{},"execution_count":5}],"source":["x = np.arange(3)\n","x"]},{"cell_type":"markdown","id":"2792609c","metadata":{"origin_pos":9,"id":"2792609c"},"source":["We can refer to an element of a vector by using a subscript.\n","For example, $x_2$ denotes the second element of $\\mathbf{x}$. \n","Since $x_2$ is a scalar, we do not bold it.\n","By default, we visualize vectors \n","by stacking their elements vertically.\n","\n","$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n","\n","Here $x_1, \\ldots, x_n$ are elements of the vector.\n","Later on, we will distinguish between such *column vectors*\n","and *row vectors* whose elements are stacked horizontally.\n","Recall that [**we access a tensor's elements via indexing.**]\n"]},{"cell_type":"code","execution_count":6,"id":"f6722ca7","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.472643Z","iopub.status.busy":"2022-09-07T22:46:31.472213Z","iopub.status.idle":"2022-09-07T22:46:31.477782Z","shell.execute_reply":"2022-09-07T22:46:31.477069Z"},"origin_pos":10,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"f6722ca7","executionInfo":{"status":"ok","timestamp":1668005602174,"user_tz":-60,"elapsed":32,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"d2eed24f-3703-4b0f-ae23-005c139e3f3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":6}],"source":["x[2]"]},{"cell_type":"markdown","id":"20c84c8a","metadata":{"origin_pos":13,"id":"20c84c8a"},"source":["To indicate that a vector contains $n$ elements,\n","we write $\\mathbf{x} \\in \\mathbb{R}^n$.\n","Formally, we call $n$ the *dimensionality* of the vector.\n","[**In code, this corresponds to the tensor's length**],\n","accessible via Python's built-in `len` function.\n"]},{"cell_type":"code","execution_count":7,"id":"7ade49f6","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.482460Z","iopub.status.busy":"2022-09-07T22:46:31.482035Z","iopub.status.idle":"2022-09-07T22:46:31.486931Z","shell.execute_reply":"2022-09-07T22:46:31.486188Z"},"origin_pos":14,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"7ade49f6","executionInfo":{"status":"ok","timestamp":1668005602174,"user_tz":-60,"elapsed":30,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"3395e68a-5ee7-4efd-cf4b-6cac81498cde"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":7}],"source":["len(x)"]},{"cell_type":"markdown","id":"51e256f0","metadata":{"origin_pos":17,"id":"51e256f0"},"source":["We can also access the length via the `shape` attribute.\n","The shape is a tuple that indicates a tensor's length along each axis.\n","(**Tensors with just one axis have shapes with just one element.**)\n"]},{"cell_type":"code","execution_count":8,"id":"1d15e69e","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.491924Z","iopub.status.busy":"2022-09-07T22:46:31.491507Z","iopub.status.idle":"2022-09-07T22:46:31.496274Z","shell.execute_reply":"2022-09-07T22:46:31.495574Z"},"origin_pos":18,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"1d15e69e","executionInfo":{"status":"ok","timestamp":1668005602174,"user_tz":-60,"elapsed":28,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"382b5a04-0337-4e03-e5c4-0c18a53cc8db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3,)"]},"metadata":{},"execution_count":8}],"source":["x.shape"]},{"cell_type":"markdown","id":"8155b5cc","metadata":{"origin_pos":21,"id":"8155b5cc"},"source":["Oftentimes, the word \"dimension\" gets overloaded\n","to mean both the number of axes \n","and the length along a particular axis.\n","To avoid this confusion, \n","we use *order* to refer to the number of axes\n","and *dimensionality* exclusively to refer \n","to the number of components.\n","\n","\n","## Matrices\n","\n","Just as scalars are $0^{\\mathrm{th}}$-order tensors\n","and vectors are $1^{\\mathrm{st}}$-order tensors,\n","matrices are $2^{\\mathrm{nd}}$-order tensors.\n","We denote matrices by bold capital letters\n","(e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$),\n","and represent them in code by tensors with two axes.\n","The expression $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n","indicates that a matrix $\\mathbf{A}$ \n","contains $m \\times n$ real-valued scalars,\n","arranged as $m$ rows and $n$ columns.\n","When $m = n$, we say that a matrix is *square*.\n","Visually, we can illustrate any matrix as a table.\n","To refer to an individual element,\n","we subscript both the row and column indices, e.g.,\n","$a_{ij}$ is the value that belongs to $\\mathbf{A}$'s\n","$i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n","\n","$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n","\n","\n","In code, we represent a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n","by a $2^{\\mathrm{nd}}$-order tensor with shape ($m$, $n$).\n","[**We can convert any appropriately sized $m \\times n$ tensor \n","into an $m \\times n$ matrix**] \n","by passing the desired shape to `reshape`:\n"]},{"cell_type":"code","execution_count":9,"id":"b6d09afd","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.501170Z","iopub.status.busy":"2022-09-07T22:46:31.500755Z","iopub.status.idle":"2022-09-07T22:46:31.506983Z","shell.execute_reply":"2022-09-07T22:46:31.506220Z"},"origin_pos":22,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"b6d09afd","executionInfo":{"status":"ok","timestamp":1668005602175,"user_tz":-60,"elapsed":26,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"88bba830-4547-4a1d-a7d3-7ecc35edc4a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1],\n","       [2, 3],\n","       [4, 5]])"]},"metadata":{},"execution_count":9}],"source":["A = np.arange(6).reshape(3, 2)\n","A"]},{"cell_type":"markdown","id":"1bff7b6e","metadata":{"origin_pos":25,"id":"1bff7b6e"},"source":["Sometimes, we want to flip the axes.\n","When we exchange a matrix's rows and columns,\n","the result is called its *transpose*.\n","Formally, we signify a matrix $\\mathbf{A}$'s transpose \n","by $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$, \n","then $b_{ij} = a_{ji}$ for all $i$ and $j$.\n","Thus, the transpose of an $m \\times n$ matrix \n","is an $n \\times m$ matrix:\n","\n","$$\n","\\mathbf{A}^\\top =\n","\\begin{bmatrix}\n","    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n","    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n","    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n","    a_{1n} & a_{2n} & \\dots  & a_{mn}\n","\\end{bmatrix}.\n","$$\n","\n","In code, we can access any (**matrix's transpose**) as follows:\n"]},{"cell_type":"code","execution_count":10,"id":"cb84d7d0","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.511273Z","iopub.status.busy":"2022-09-07T22:46:31.510843Z","iopub.status.idle":"2022-09-07T22:46:31.516916Z","shell.execute_reply":"2022-09-07T22:46:31.516064Z"},"origin_pos":26,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"cb84d7d0","executionInfo":{"status":"ok","timestamp":1668005602175,"user_tz":-60,"elapsed":22,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"b291bd69-b785-4ae6-b90c-8615e65c9f3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 2, 4],\n","       [1, 3, 5]])"]},"metadata":{},"execution_count":10}],"source":["A.T"]},{"cell_type":"markdown","id":"6b20d05c","metadata":{"origin_pos":29,"id":"6b20d05c"},"source":["[**Symmetric matrices are the subset of square matrices\n","that are equal to their own transposes:\n","$\\mathbf{A} = \\mathbf{A}^\\top$.**]\n","The following matrix is symmetric:\n"]},{"cell_type":"code","execution_count":11,"id":"c6de8f00","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.520276Z","iopub.status.busy":"2022-09-07T22:46:31.519827Z","iopub.status.idle":"2022-09-07T22:46:31.526634Z","shell.execute_reply":"2022-09-07T22:46:31.525926Z"},"origin_pos":30,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"c6de8f00","executionInfo":{"status":"ok","timestamp":1668005602175,"user_tz":-60,"elapsed":20,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"8553e2d2-1ff4-4d18-835a-5322cf30c961"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ True,  True,  True],\n","       [ True,  True,  True],\n","       [ True,  True,  True]])"]},"metadata":{},"execution_count":11}],"source":["A = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n","A == A.T"]},{"cell_type":"markdown","id":"1cba5428","metadata":{"origin_pos":33,"id":"1cba5428"},"source":["Matrices are useful for representing datasets. \n","Typically, rows correspond to individual records\n","and columns correspond to distinct attributes.\n","\n","\n","\n","## Tensors\n","\n","While you can go far in your machine learning journey\n","with only scalars, vectors, and matrices,\n","eventually you may need to work with \n","higher-order [**tensors**].\n","Tensors (**give us a generic way to describe \n","extensions to $n^{\\mathrm{th}}$-order arrays.**)\n","We call software objects of the *tensor class* \"tensors\"\n","precisely because they too can have arbitrary numbers of axes.\n","While it may be confusing to use the word\n","*tensor* for both the mathematical object\n","and its realization in code,\n","our meaning should usually be clear from context.\n","We denote general tensors by capital letters \n","with a special font face\n","(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)\n","and their indexing mechanism \n","(e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) \n","follows naturally from that of matrices.\n","\n","Tensors will become more important \n","when we start working with images.\n","Each image arrives as a $3^{\\mathrm{rd}}$-order tensor\n","with axes corresponding to the height, width, and *channel*.\n","At each spatial location, the intensities \n","of each color (red, green, and blue)\n","are stacked along the channel. \n","Moreover a collection of images is represented \n","in code by a $4^{\\mathrm{th}}$-order tensor,\n","where distinct images are indexed\n","along the first axis.\n","Higher-order tensors are constructed analogously \n","to vectors and matrices,\n","by growing the number of shape components.\n"]},{"cell_type":"code","execution_count":12,"id":"20cfb52d","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.530318Z","iopub.status.busy":"2022-09-07T22:46:31.529628Z","iopub.status.idle":"2022-09-07T22:46:31.536059Z","shell.execute_reply":"2022-09-07T22:46:31.535353Z"},"origin_pos":34,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"20cfb52d","executionInfo":{"status":"ok","timestamp":1668005602175,"user_tz":-60,"elapsed":18,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"2ea22994-363d-450b-f365-cda9112b74f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]],\n","\n","       [[12, 13, 14, 15],\n","        [16, 17, 18, 19],\n","        [20, 21, 22, 23]]])"]},"metadata":{},"execution_count":12}],"source":["np.arange(24).reshape(2, 3, 4)"]},{"cell_type":"markdown","id":"f418d6df","metadata":{"origin_pos":37,"id":"f418d6df"},"source":["## Basic Properties of Tensor Arithmetic\n","\n","Scalars, vectors, matrices, \n","and higher-order tensors\n","all have some handy properties. \n","For example, elementwise operations\n","produce outputs that have the \n","same shape as their operands.\n"]},{"cell_type":"code","execution_count":13,"id":"e8f1f5bc","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.539525Z","iopub.status.busy":"2022-09-07T22:46:31.539064Z","iopub.status.idle":"2022-09-07T22:46:31.548473Z","shell.execute_reply":"2022-09-07T22:46:31.547739Z"},"origin_pos":38,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"e8f1f5bc","executionInfo":{"status":"ok","timestamp":1668005602176,"user_tz":-60,"elapsed":17,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"01580b2d-834a-417d-b9b3-d7da18f0b3e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 2],\n","       [3, 4, 5]])"]},"metadata":{},"execution_count":13}],"source":["A = np.arange(6).reshape(2, 3)\n","A"]},{"cell_type":"code","source":["B = A.copy()  # Assign a copy of `A` to `B` by allocating new memory\n","A + B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVmY9B4HyvSJ","executionInfo":{"status":"ok","timestamp":1668005602176,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"981c6686-9499-4e6f-e705-32968cb6a2ee"},"id":"VVmY9B4HyvSJ","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  2,  4],\n","       [ 6,  8, 10]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","id":"e41c4619","metadata":{"origin_pos":41,"id":"e41c4619"},"source":["The [**elementwise product of two matrices\n","is called their *Hadamard product***] (denoted $\\odot$).\n","Below, we spell out the entries \n","of the Hadamard product of two matrices \n","$\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$:\n","\n","\n","\n","$$\n","\\mathbf{A} \\odot \\mathbf{B} =\n","\\begin{bmatrix}\n","    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n","    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n","    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n","\\end{bmatrix}.\n","$$\n"]},{"cell_type":"code","execution_count":15,"id":"b5f61577","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.551698Z","iopub.status.busy":"2022-09-07T22:46:31.551261Z","iopub.status.idle":"2022-09-07T22:46:31.556661Z","shell.execute_reply":"2022-09-07T22:46:31.555931Z"},"origin_pos":42,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"b5f61577","executionInfo":{"status":"ok","timestamp":1668005602176,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"e13c4d54-56e1-48f7-a20d-fb29f9d82292"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  1,  4],\n","       [ 9, 16, 25]])"]},"metadata":{},"execution_count":15}],"source":["A * B"]},{"cell_type":"markdown","id":"16e66cd1","metadata":{"origin_pos":45,"id":"16e66cd1"},"source":["[**Adding or multiplying a scalar and a tensor**] produces a result\n","with the same shape as the original tensor.\n","Here, each element of the tensor is added to (or multiplied by) the scalar.\n"]},{"cell_type":"code","execution_count":16,"id":"59b4c39a","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.559889Z","iopub.status.busy":"2022-09-07T22:46:31.559484Z","iopub.status.idle":"2022-09-07T22:46:31.566290Z","shell.execute_reply":"2022-09-07T22:46:31.565535Z"},"origin_pos":46,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"59b4c39a","executionInfo":{"status":"ok","timestamp":1668005602176,"user_tz":-60,"elapsed":11,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"8a48e171-91ad-4b44-9634-9b9f811d4817"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[[ 2,  3,  4,  5],\n","         [ 6,  7,  8,  9],\n","         [10, 11, 12, 13]],\n"," \n","        [[14, 15, 16, 17],\n","         [18, 19, 20, 21],\n","         [22, 23, 24, 25]]]), (2, 3, 4))"]},"metadata":{},"execution_count":16}],"source":["a = 2\n","X = np.arange(24).reshape(2, 3, 4)\n","a + X, (a + X).shape"]},{"cell_type":"code","source":["a * X, (a * X).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJaqV0m7zJTY","executionInfo":{"status":"ok","timestamp":1668005602711,"user_tz":-60,"elapsed":543,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"d0ff1e94-6175-4e54-80a3-2e2c85f3a17e"},"id":"KJaqV0m7zJTY","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[[ 0,  2,  4,  6],\n","         [ 8, 10, 12, 14],\n","         [16, 18, 20, 22]],\n"," \n","        [[24, 26, 28, 30],\n","         [32, 34, 36, 38],\n","         [40, 42, 44, 46]]]), (2, 3, 4))"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","id":"065a2ce3","metadata":{"origin_pos":49,"id":"065a2ce3"},"source":["## Reduction\n","\n","Often, we wish to calculate [**the sum of a tensor's elements.**]\n","To express the sum of the elements in a vector $\\mathbf{x}$ of length $n$,\n","we write $\\sum_{i=1}^n x_i$. There's a simple function for it:\n"]},{"cell_type":"code","execution_count":18,"id":"a717d7df","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.569552Z","iopub.status.busy":"2022-09-07T22:46:31.569138Z","iopub.status.idle":"2022-09-07T22:46:31.576911Z","shell.execute_reply":"2022-09-07T22:46:31.576167Z"},"origin_pos":50,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"a717d7df","executionInfo":{"status":"ok","timestamp":1668005602712,"user_tz":-60,"elapsed":41,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"778623f6-e9f8-4280-8d63-7747c5cbc8aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0, 1, 2]), 3)"]},"metadata":{},"execution_count":18}],"source":["x = np.arange(3)\n","x, x.sum()"]},{"cell_type":"markdown","id":"0fba8ad2","metadata":{"origin_pos":53,"id":"0fba8ad2"},"source":["To express [**sums over the elements of tensors of arbitrary shape**],\n","we simply sum over all of its axes. \n","For example, the sum of the elements \n","of an $m \\times n$ matrix $\\mathbf{A}$ \n","could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$.\n"]},{"cell_type":"code","source":["A"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1ltdneczSuh","executionInfo":{"status":"ok","timestamp":1668005602712,"user_tz":-60,"elapsed":40,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"66c07c0f-a53a-4a5e-d13f-a5e7f1eb5643"},"id":"z1ltdneczSuh","execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 2],\n","       [3, 4, 5]])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","execution_count":20,"id":"cf5521b3","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.580533Z","iopub.status.busy":"2022-09-07T22:46:31.579887Z","iopub.status.idle":"2022-09-07T22:46:31.586536Z","shell.execute_reply":"2022-09-07T22:46:31.585745Z"},"origin_pos":54,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"cf5521b3","executionInfo":{"status":"ok","timestamp":1668005602712,"user_tz":-60,"elapsed":37,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"7612f1f5-8da7-4f70-a9e1-2bec14517599"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2, 3), 15)"]},"metadata":{},"execution_count":20}],"source":["A.shape, A.sum()"]},{"cell_type":"markdown","id":"541cac06","metadata":{"origin_pos":57,"id":"541cac06"},"source":["By default, invoking the sum function\n","*reduces* a tensor along all of its axes,\n","eventually producing a scalar.\n","Our libraries also allow us to [**specify the axes \n","along which the tensor should be reduced.**]\n","To sum over all elements along the rows (axis 0),\n","we specify `axis=0` in `sum`.\n","Since the input matrix reduces along axis 0\n","to generate the output vector,\n","this axis is missing from the shape of the output.\n"]},{"cell_type":"code","execution_count":21,"id":"58335059","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.589885Z","iopub.status.busy":"2022-09-07T22:46:31.589388Z","iopub.status.idle":"2022-09-07T22:46:31.595143Z","shell.execute_reply":"2022-09-07T22:46:31.594389Z"},"origin_pos":58,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"58335059","executionInfo":{"status":"ok","timestamp":1668005602713,"user_tz":-60,"elapsed":36,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"8fda2003-2b3c-43d4-ac2b-b01be131ee7e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([3, 5, 7]), (3,))"]},"metadata":{},"execution_count":21}],"source":["A.sum(axis=0), A.sum(axis=0).shape"]},{"cell_type":"markdown","id":"c886eb80","metadata":{"origin_pos":61,"id":"c886eb80"},"source":["Specifying `axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns.\n"]},{"cell_type":"code","execution_count":22,"id":"4828704f","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.598738Z","iopub.status.busy":"2022-09-07T22:46:31.598170Z","iopub.status.idle":"2022-09-07T22:46:31.604628Z","shell.execute_reply":"2022-09-07T22:46:31.603777Z"},"origin_pos":62,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"4828704f","executionInfo":{"status":"ok","timestamp":1668005602713,"user_tz":-60,"elapsed":35,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"02352d70-f603-4cea-eaf0-2f71333f32df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 3, 12]), (2,))"]},"metadata":{},"execution_count":22}],"source":["A.sum(axis=1), A.sum(axis=1).shape"]},{"cell_type":"markdown","id":"98e552a4","metadata":{"origin_pos":69,"id":"98e552a4"},"source":["[**A related quantity is the *mean*, also called the *average*.**]\n","We calculate the mean by dividing the sum \n","by the total number of elements.\n","Because computing the mean is so common,\n","it gets a dedicated library function \n","that works analogously to `sum`.\n"]},{"cell_type":"code","execution_count":23,"id":"b15b7bbd","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.617591Z","iopub.status.busy":"2022-09-07T22:46:31.617158Z","iopub.status.idle":"2022-09-07T22:46:31.623746Z","shell.execute_reply":"2022-09-07T22:46:31.623040Z"},"origin_pos":70,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"b15b7bbd","executionInfo":{"status":"ok","timestamp":1668005602713,"user_tz":-60,"elapsed":32,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"a0838815-3f63-4cba-9968-fc8e539bc2b4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.5"]},"metadata":{},"execution_count":23}],"source":["A.mean()"]},{"cell_type":"code","source":["A.sum() / A.size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSNYkdI80AiR","executionInfo":{"status":"ok","timestamp":1668005602713,"user_tz":-60,"elapsed":30,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"a285d7b7-1476-452b-e50e-a93f56892c92"},"id":"KSNYkdI80AiR","execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.5"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","id":"8d7ebae9","metadata":{"origin_pos":89,"id":"8d7ebae9"},"source":["## Dot Products\n","\n","So far, we have only performed elementwise operations, sums, and averages. \n","And if this was all we could do, linear algebra \n","would not deserve its own section.\n","Fortunately, this is where things get more interesting.\n","One of the most fundamental operations is the dot product.\n","Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$,\n","their *dot product* $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$) \n","is a sum over the products of the elements at the same position: \n","$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$.\n","\n","[The *dot product* of two vectors is a sum over the products of the elements at the same position]\n"]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4RC4XCM1wui","executionInfo":{"status":"ok","timestamp":1668005602713,"user_tz":-60,"elapsed":28,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"37153fbf-f580-431a-ada5-f06a8901a8f5"},"id":"I4RC4XCM1wui","execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":26,"id":"938b94ad","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.668917Z","iopub.status.busy":"2022-09-07T22:46:31.668269Z","iopub.status.idle":"2022-09-07T22:46:31.678003Z","shell.execute_reply":"2022-09-07T22:46:31.676750Z"},"origin_pos":90,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"938b94ad","executionInfo":{"status":"ok","timestamp":1668005602714,"user_tz":-60,"elapsed":26,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"74718898-949d-435a-d4cb-7e0a2bb8fd54"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.])"]},"metadata":{},"execution_count":26}],"source":["y = np.ones(3)\n","y"]},{"cell_type":"code","source":["np.dot(x, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_u2vPm5o113b","executionInfo":{"status":"ok","timestamp":1668005602714,"user_tz":-60,"elapsed":24,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"ee84bb32-197c-427a-b1ee-373bbadeae70"},"id":"_u2vPm5o113b","execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.0"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","id":"80b11eae","metadata":{"origin_pos":93,"id":"80b11eae"},"source":["Equivalently, (**we can calculate the dot product of two vectors \n","by performing an elementwise multiplication followed by a sum:**)\n"]},{"cell_type":"code","execution_count":28,"id":"90340ba6","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.681812Z","iopub.status.busy":"2022-09-07T22:46:31.681164Z","iopub.status.idle":"2022-09-07T22:46:31.688505Z","shell.execute_reply":"2022-09-07T22:46:31.687542Z"},"origin_pos":94,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"90340ba6","executionInfo":{"status":"ok","timestamp":1668005602714,"user_tz":-60,"elapsed":22,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"27c78c08-eb1d-43dc-eeab-766660fd1227"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.0"]},"metadata":{},"execution_count":28}],"source":["np.sum(x * y)"]},{"cell_type":"markdown","id":"d62e521c","metadata":{"origin_pos":97,"id":"d62e521c"},"source":["Dot products are useful in a wide range of contexts.\n","For example, given some set of values,\n","denoted by a vector $\\mathbf{x}  \\in \\mathbb{R}^n$\n","and a set of weights denoted by $\\mathbf{w} \\in \\mathbb{R}^n$,\n","the weighted sum of the values in $\\mathbf{x}$\n","according to the weights $\\mathbf{w}$\n","could be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}$.\n","When the weights are non-negative\n","and sum to one, i.e., $\\left(\\sum_{i=1}^{n} {w_i} = 1\\right)$,\n","the dot product expresses a *weighted average*.\n","After normalizing two vectors to have unit length,\n","the dot products express the cosine of the angle between them.\n","Later in this section, we will formally introduce this notion of *length*.\n","\n","\n","## Matrix-Vector Products\n","\n","Now that we know how to calculate dot products,\n","we can begin to understand the *product*\n","between an $m \\times n$ matrix $\\mathbf{A}$ \n","and an $n$-dimensional vector $\\mathbf{x}$.\n","To start off, we visualize our matrix\n","in terms of its row vectors\n","\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_m \\\\\n","\\end{bmatrix},$$\n","\n","where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$\n","is a row vector representing the $i^\\mathrm{th}$ row \n","of the matrix $\\mathbf{A}$.\n","\n","[**The matrix-vector product $\\mathbf{A}\\mathbf{x}$\n","is simply a column vector of length $m$,\n","whose $i^\\mathrm{th}$ element is the dot product \n","$\\mathbf{a}^\\top_i \\mathbf{x}$:**]\n","\n","$$\n","\\mathbf{A}\\mathbf{x}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_m \\\\\n","\\end{bmatrix}\\mathbf{x}\n","= \\begin{bmatrix}\n"," \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n"," \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n","\\vdots\\\\\n"," \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n","\\end{bmatrix}.\n","$$\n","\n","We can think of multiplication with a matrix\n","$\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$\n","as a transformation that projects vectors\n","from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n","These transformations are remarkably useful.\n","For example, we can represent rotations\n","as multiplications by certain square matrices.\n","Matrix-vector products also describe \n","the key calculation involved in computing\n","the outputs of each layer in a neural network\n","given the outputs from the previous layer.\n"]},{"cell_type":"markdown","id":"bb799bb0","metadata":{"origin_pos":98,"tab":["mxnet"],"id":"bb799bb0"},"source":["To express a matrix-vector product in code,\n","we use the same `dot` function.\n","The operation is inferred \n","based on the type of the arguments.\n","Note that the column dimension of `A` \n","(its length along axis 1)\n","must be the same as the dimension of `x` (its length).\n"]},{"cell_type":"code","execution_count":29,"id":"ccb049e2","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.692415Z","iopub.status.busy":"2022-09-07T22:46:31.691860Z","iopub.status.idle":"2022-09-07T22:46:31.699018Z","shell.execute_reply":"2022-09-07T22:46:31.698086Z"},"origin_pos":101,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"ccb049e2","executionInfo":{"status":"ok","timestamp":1668005602714,"user_tz":-60,"elapsed":20,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"3562eef0-e4b3-4ab3-ab38-ac412494e846"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2, 3), (3,), array([ 5, 14]))"]},"metadata":{},"execution_count":29}],"source":["A.shape, x.shape, np.dot(A, x)"]},{"cell_type":"markdown","id":"9eb5dcc2","metadata":{"origin_pos":104,"id":"9eb5dcc2"},"source":["## Matrix-Matrix Multiplication\n","\n","If you've gotten the hang of dot products and matrix-vector products,\n","then *matrix-matrix multiplication* should be straightforward.\n","\n","Say that we have two matrices \n","$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ \n","and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n","\\end{bmatrix}.$$\n","\n","\n","Let $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ denote \n","the row vector representing the $i^\\mathrm{th}$ row \n","of the matrix $\\mathbf{A}$\n","and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ denote \n","the column vector from the $j^\\mathrm{th}$ column \n","of the matrix $\\mathbf{B}$:\n","\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix},\n","\\quad \\mathbf{B}=\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}.\n","$$\n","\n","\n","To form the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$,\n","we simply compute each element $c_{ij}$\n","as the dot product between \n","the $i^{\\mathrm{th}}$ row of $\\mathbf{A}$\n","and the $j^{\\mathrm{th}}$ row of $\\mathbf{B}$,\n","i.e., $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n","\n","$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix}\n","\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n"," \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n"," \\vdots & \\vdots & \\ddots &\\vdots\\\\\n","\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n","\\end{bmatrix}.\n","$$\n","\n","[**We can think of the matrix-matrix multiplication $\\mathbf{AB}$\n","as performing $m$ matrix-vector products \n","or $m \\times n$ dot products \n","and stitching the results together \n","to form an $n \\times m$ matrix.**]\n","In the following snippet, \n","we perform matrix multiplication on `A` and `B`.\n","Here, `A` is a matrix with 2 rows and 3 columns,\n","and `B` is a matrix with 3 rows and 4 columns.\n","After multiplication, we obtain a matrix with 2 rows and 4 columns.\n"]},{"cell_type":"code","source":["A"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gDL_WTB2MQq","executionInfo":{"status":"ok","timestamp":1668005602714,"user_tz":-60,"elapsed":18,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"fe900aea-ad97-4b18-e0ed-4a585996bc52"},"id":"7gDL_WTB2MQq","execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 2],\n","       [3, 4, 5]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","execution_count":31,"id":"61566330","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.702921Z","iopub.status.busy":"2022-09-07T22:46:31.702260Z","iopub.status.idle":"2022-09-07T22:46:31.709609Z","shell.execute_reply":"2022-09-07T22:46:31.708728Z"},"origin_pos":105,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"61566330","executionInfo":{"status":"ok","timestamp":1668005602715,"user_tz":-60,"elapsed":17,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"031e9e9f-3be0-438c-ab9a-5e39b66f2aa5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3.,  3.,  3.,  3.],\n","       [12., 12., 12., 12.]])"]},"metadata":{},"execution_count":31}],"source":["B = np.ones(shape=(3, 4))\n","np.dot(A, B)"]},{"cell_type":"markdown","id":"5cd3149c","metadata":{"origin_pos":108,"id":"5cd3149c"},"source":["The term *matrix-matrix multiplication* is \n","often simplified to *matrix multiplication*,\n","and should not be confused with the Hadamard product.\n","\n","\n","## Norms\n","\n","Some of the most useful operators in linear algebra are *norms*.\n","Informally, the norm of a vector tells us how *big* it is. \n","For instance, the $\\\\ell_2$ norm measures\n","the (Euclidean) length of a vector.\n","Here, we are employing a notion of *size* that concerns the magnitude a vector's components\n","(not its dimensionality). \n","\n","A norm is a function $\\| \\cdot \\|$ that maps a vector\n","to a scalar and satisfies the following three properties:\n","\n","1. Given any vector $\\mathbf{x}$, if we scale (all elements of) the vector \n","   by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n","   $$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$$\n","2. For any vectors $\\mathbf{x}$ and $\\mathbf{y}$:\n","   norms satisfy the triangle inequality:\n","   $$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$$\n","3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n","   $$\\|\\mathbf{x}\\| > 0 \\text{ for all } \\mathbf{x} \\neq 0.$$\n","\n","Many functions are valid norms and different norms \n","encode different notions of size.\n"]},{"cell_type":"markdown","source":["### Euclidean norm\n","The Euclidean norm that we all learned in elementary school geometry\n","when calculating the hypotenuse of right triangle\n","is the square root of the sum of squares of a vector's elements.\n","Formally, this is called [**the $\\ell_2$ *norm***] and expressed as\n","\n","(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$$**)\n","\n","The method `norm` calculates the $\\ell_2$ norm."],"metadata":{"id":"BnkJPj8d3Y_R"},"id":"BnkJPj8d3Y_R"},{"cell_type":"code","execution_count":32,"id":"b316e53d","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.713297Z","iopub.status.busy":"2022-09-07T22:46:31.712736Z","iopub.status.idle":"2022-09-07T22:46:31.720757Z","shell.execute_reply":"2022-09-07T22:46:31.719859Z"},"origin_pos":109,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"b316e53d","executionInfo":{"status":"ok","timestamp":1668005602715,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"44d7bf9c-413a-4ed6-d540-b872e7fa00c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.0"]},"metadata":{},"execution_count":32}],"source":["u = np.array([3, -4])\n","np.linalg.norm(u)"]},{"cell_type":"code","source":["# Autre forme de faire"],"metadata":{"id":"PFsyGwXG2jip","executionInfo":{"status":"ok","timestamp":1668005602715,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}}},"id":"PFsyGwXG2jip","execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["### Abs norm"],"metadata":{"id":"JCOmn6qP3TVR"},"id":"JCOmn6qP3TVR"},{"cell_type":"markdown","id":"c84e2fec","metadata":{"origin_pos":112,"id":"c84e2fec"},"source":["[**The $\\ell_1$ norm**] is also popular \n","and the associated metric is called the Manhattan distance. \n","By definition, the $\\ell_1$ norm sums \n","the absolute values of a vector's elements:\n","\n","(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n","\n","Compared to the $\\ell_2$ norm, it is less sensitive to outliers.\n","To compute the $\\ell_1$ norm, \n","we compose the absolute value\n","with the sum operation.\n"]},{"cell_type":"code","source":["u"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOG_Hy3E3fjs","executionInfo":{"status":"ok","timestamp":1668005602715,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"28a923c2-815a-4878-cbd0-5ad11544483b"},"id":"UOG_Hy3E3fjs","execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3, -4])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","execution_count":35,"id":"51ba3f12","metadata":{"execution":{"iopub.execute_input":"2022-09-07T22:46:31.725818Z","iopub.status.busy":"2022-09-07T22:46:31.725060Z","iopub.status.idle":"2022-09-07T22:46:31.731999Z","shell.execute_reply":"2022-09-07T22:46:31.731111Z"},"origin_pos":113,"tab":["mxnet"],"colab":{"base_uri":"https://localhost:8080/"},"id":"51ba3f12","executionInfo":{"status":"ok","timestamp":1668005602715,"user_tz":-60,"elapsed":11,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"a3e4b647-de00-45a9-d4f6-1f2d6f55103e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 4])"]},"metadata":{},"execution_count":35}],"source":["np.abs(u)"]},{"cell_type":"code","source":["np.abs(u).sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcPoyDBs3hgh","executionInfo":{"status":"ok","timestamp":1668005602716,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}},"outputId":"832a8930-f178-4c13-ac27-c5e8f1d8b686"},"id":"GcPoyDBs3hgh","execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["### RMSE"],"metadata":{"id":"4MNXvnIQ30lJ"},"id":"4MNXvnIQ30lJ"},{"cell_type":"code","source":[],"metadata":{"id":"2pfrK9OY34vT","executionInfo":{"status":"ok","timestamp":1668005602716,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hub-Maths UniLaSalle","userId":"01104378893967613228"}}},"id":"2pfrK9OY34vT","execution_count":36,"outputs":[]},{"cell_type":"markdown","id":"7f5e0e88","metadata":{"origin_pos":116,"id":"7f5e0e88"},"source":["Both the $\\ell_2$ and $\\ell_1$ norms are special cases\n","of the more general $\\ell_p$ *norms*:\n","\n","$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n","\n","In the case of matrices, matters are more complicated. \n","After all, matrices can be viewed both as collections of individual entries \n","*and* as objects that operate on vectors and transform them into other vectors. \n","For instance, we can ask by how much longer \n","the matrix-vector product $\\mathbf{X} \\mathbf{v}$ \n","could be relative to $\\mathbf{v}$. \n","This line of thought leads to a norm called the *spectral* norm. \n","For now, we introduce [**the *Frobenius norm*, \n","which is much easier to compute**] and defined as\n","the square root of the sum of the squares \n","of a matrix's elements:\n","\n","[**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**]\n","\n","The Frobenius norm behaves as if it were \n","an $\\ell_2$ norm of a matrix-shaped vector.\n","Invoking the following function will calculate \n","the Frobenius norm of a matrix.\n"]},{"cell_type":"markdown","id":"007b90a2","metadata":{"origin_pos":120,"id":"007b90a2"},"source":["## Exercises\n","\n","1. Prove that the transpose of the transpose of a matrix is the matrix itself: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n","1. Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that sum and transposition commute: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$.\n","1. Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Can you prove the result by using only the result of the previous two exercises?\n","1. We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code. \n","1. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?\n","1. Run `A / A.sum(axis=1)` and see what happens. Can you analyze the reason?\n","1. When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?\n","1. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along axis 0, 1, and 2?\n","1. Feed a tensor with 3 or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?\n","1. Define three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{14}}$, for instance initialized with Gaussian random variables. You want to compute the product $\\mathbf{A} \\mathbf{B} \\mathbf{C}$. Is there any difference in memory footprint and speed, depending on whether you compute $(\\mathbf{A} \\mathbf{B}) \\mathbf{C}$ or $\\mathbf{A} (\\mathbf{B} \\mathbf{C})$. Why?\n","1. Define three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$. Is there any difference in speed depending on whether you compute $\\mathbf{A} \\mathbf{B}$ or $\\mathbf{A} \\mathbf{C}^\\top$? Why? What changes if you initialize $\\mathbf{C} = \\mathbf{B}^\\top$ without cloning memory? Why?\n","1. Define three matrices, say $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200}$. Constitute a tensor with 3 axes by stacking $[\\mathbf{A}, \\mathbf{B}, \\mathbf{C}]$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $\\mathbf{B}$. Check that your answer is correct.\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}