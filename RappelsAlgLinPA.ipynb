{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RappelsAlgLinPA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9wQLMaBvlz"
      },
      "source": [
        "# PA Numérique - Machine Learning\n",
        "\n",
        "\n",
        "**Rappels d'Algèbre Linéaire**\n",
        "\n",
        "**Hamilton Araujo**\n",
        "\n",
        "22 octobre 2021\n",
        "\n",
        "- Lien vers le cours associé a ce notebook: https://www.dropbox.com/s/p31gnwymng07efn/RappelsAlgLinPA.pdf\n",
        "\n",
        "- Info: Ce notebook est en Anglais avec certains commentaires en Français.\n",
        "\n",
        "- Source: https://d2l.ai/chapter_preliminaries/linear-algebra.html\n",
        "\n",
        "## Imports\n",
        "\n",
        "- Nous allons utilizer la librairie de deep learning PyTorch.\n",
        "\n",
        "- PyTorch permet de manipuler des tenseurs (tableaux multidimensionnels), de les échanger facilement avec Numpy et d'effectuer des calculs efficaces sur CPU ou GPU (par exemple des produits de matrices);\n",
        "- Calculer des gradients pour appliquer facilement des algorithmes d'optimisation par descente de gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4iOQ2pzIvIz"
      },
      "source": [
        "# Utilisez la ligne de code ci-dessous avant de faire l'import\n",
        "# conda install pytorch torchvision -c pytorch\n",
        "#conda install pytorch-cpu torchvision-cpu -c pytorch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDDySHXpBvl0"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY748iruwerR",
        "origin_pos": 0
      },
      "source": [
        "## Scalars\n",
        "\n",
        "If you never studied linear algebra or machine learning,\n",
        "then your past experience with math probably consisted\n",
        "of thinking about one number at a time.\n",
        "And, if you ever balanced a checkbook\n",
        "or even paid for dinner at a restaurant\n",
        "then you already know how to do basic things\n",
        "like adding and multiplying pairs of numbers.\n",
        "For example, the temperature in Palo Alto is $52$ degrees Fahrenheit.\n",
        "Formally, we call values consisting\n",
        "of just one numerical quantity *scalars*.\n",
        "If you wanted to convert this value to Celsius\n",
        "(the metric system's more sensible temperature scale),\n",
        "you would evaluate the expression $c = \\frac{5}{9}(f - 32)$, setting $f$ to $52$.\n",
        "In this equation, each of the terms---$5$, $9$, and $32$---are scalar values.\n",
        "The placeholders $c$ and $f$ are called *variables*\n",
        "and they represent unknown scalar values.\n",
        "\n",
        "In this book, we adopt the mathematical notation\n",
        "where scalar variables are denoted\n",
        "by ordinary lower-cased letters (e.g., $x$, $y$, and $z$).\n",
        "We denote the space of all (continuous) *real-valued* scalars by $\\mathbb{R}$.\n",
        "For expedience, we will punt on rigorous definitions\n",
        "of what precisely *space* is,\n",
        "but just remember for now that the expression $x \\in \\mathbb{R}$\n",
        "is a formal way to say that $x$ is a real-valued scalar.\n",
        "The symbol $\\in$ can be pronounced \"in\"\n",
        "and simply denotes membership in a set.\n",
        "Analogously, we could write $x, y \\in \\{0, 1\\}$\n",
        "to state that $x$ and $y$ are numbers\n",
        "whose value can only be $0$ or $1$.\n",
        "\n",
        "**A scalar is represented by a tensor with just one element.**\n",
        "\n",
        "In the next snippet, we instantiate two scalars\n",
        "and perform some familiar arithmetic operations with them,\n",
        "namely addition, multiplication, division, and exponentiation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PvlKjwrwerT",
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "# Définir deux scalaires\n",
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr0QrxMsBvl3",
        "outputId": "c360639e-f2a8-4fb3-a265-3832c9d8948f"
      },
      "source": [
        "# Afficher x\n",
        "x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZt63_BIPR6A",
        "outputId": "766cf3f6-5677-4083-9e93-e367e8e66686"
      },
      "source": [
        "# Afficher y autrement\n",
        "print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8y3Ra5ZBvl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ffd443f-4aa0-41f6-e4b6-e54d495ff41f"
      },
      "source": [
        "# Opérations avec x et y\n",
        "x + y, x * y, x / y, x**y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XeEMU1MBvl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40273c57-d667-4b28-f388-3bf66f0301ed"
      },
      "source": [
        "# Autres opérations\n",
        "x + 3 , x**0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(6.), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UngFRX6LBvl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5a18a2-797a-4da0-bc6e-e140123e8a31"
      },
      "source": [
        "#Sans le .0 ça marche!\n",
        "z = torch.tensor(7)\n",
        "z"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEo3Xf9MMxgl"
      },
      "source": [
        "# Retour cours !"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-nJF5gfwerU",
        "origin_pos": 4
      },
      "source": [
        "## Vectors\n",
        "\n",
        "**You can think of a vector as simply a list of scalar values.**\n",
        "\n",
        "We call these values the *elements* (*entries* or *components*) of the vector.\n",
        "When our vectors represent examples from our dataset,\n",
        "their values hold some real-world significance.\n",
        "For example, if we were training a model to predict\n",
        "the risk that a loan defaults,\n",
        "we might associate each applicant with a vector\n",
        "whose components correspond to their income,\n",
        "length of employment, number of previous defaults, and other factors.\n",
        "If we were studying the risk of heart attacks hospital patients potentially face,\n",
        "we might represent each patient by a vector\n",
        "whose components capture their most recent vital signs,\n",
        "cholesterol levels, minutes of exercise per day, etc.\n",
        "In math notation, we will usually denote vectors as bold-faced,\n",
        "lower-cased letters (e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z})$.\n",
        "\n",
        "We work with vectors via one-dimensional tensors.\n",
        "In general tensors can have arbitrary lengths,\n",
        "subject to the memory limits of your machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6HFDnfQwerV",
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "# On définit un vecteur de dimension 4\n",
        "u = torch.arange(4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zha6yYCVBvl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd28a3d7-ef2c-4160-b05d-8d5bd409710c"
      },
      "source": [
        "# On affiche u\n",
        "print(u)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue94z9J9werV",
        "origin_pos": 8
      },
      "source": [
        "We can refer to any element of a vector by using a subscript.\n",
        "For example, we can refer to the $i^\\mathrm{th}$ element of $\\mathbf{x}$ by $x_i$.\n",
        "Note that the element $x_i$ is a scalar,\n",
        "so we do not bold-face the font when referring to it.\n",
        "Extensive literature considers column vectors to be the default\n",
        "orientation of vectors, so does this book.\n",
        "In math, a vector $\\mathbf{x}$ can be written as\n",
        "\n",
        "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
        "\n",
        "\n",
        "\n",
        "where $x_1, \\ldots, x_n$ are elements of the vector.\n",
        "In code,\n",
        "we **access any element by indexing into the tensor.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaGO4Or3werV",
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4181f8f-fb5d-41b0-dcc2-764e11e47ca1"
      },
      "source": [
        "# Afficher u et son 4ème terme \n",
        "u, u[3]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3]), tensor(3))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDbvP75QUvlt",
        "outputId": "459ccda4-6b35-4bf1-fab8-3604f66ed8a8"
      },
      "source": [
        "# Après avoir accédé à un terme, nous pouvons exécuter des opérations avec.\n",
        "u[3] - 1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICS8Sw3_LuVk"
      },
      "source": [
        "# Question: comment définir un vecteur avec les élémente que je veux ?  (.tensor([]))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Et6makwerW",
        "origin_pos": 12
      },
      "source": [
        "### Length, Dimensionality, and Shape\n",
        "\n",
        "A vector is just an array of numbers.\n",
        "And just as every array has a length, so does every vector.\n",
        "In math notation, if we want to say that a vector $\\mathbf{x}$\n",
        "consists of $n$ real-valued scalars,\n",
        "we can express this as $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
        "The length of a vector is commonly called the *dimension* of the vector.\n",
        "\n",
        "As with an ordinary Python array,\n",
        "we **can access the length of a tensor**\n",
        "by calling Python's built-in `len()` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IttVLcC7werW",
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46840597-16ff-4086-d71b-f5a815f40c1a"
      },
      "source": [
        "# Dimension de u (longueur de la 'liste' u)\n",
        "len(u)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTG56-dvwerX",
        "origin_pos": 16
      },
      "source": [
        "When a tensor represents a vector (with precisely one axis),\n",
        "we can also access its length via the `.shape` attribute.\n",
        "The shape is a tuple that lists the length (dimensionality)\n",
        "along each axis of the tensor.``\n",
        "\n",
        "**For tensors with just one axis, the shape has just one element.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmFXmeCAwerX",
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e83bd4b-2586-4b29-fa36-4417abf219ce"
      },
      "source": [
        "# Forme de u\n",
        "u.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S09zdbtQMtD0"
      },
      "source": [
        "# Retour cours !"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQrqeBk_werX",
        "origin_pos": 20
      },
      "source": [
        "Note that the word \"dimension\" tends to get overloaded\n",
        "in these contexts and this tends to confuse people.\n",
        "To clarify, we use the dimensionality of a *vector* or an *axis*\n",
        "to refer to its length, i.e., the number of elements of a vector or an axis.\n",
        "However, we use the dimensionality of a tensor\n",
        "to refer to the number of axes that a tensor has.\n",
        "In this sense, the dimensionality of some axis of a tensor\n",
        "will be the length of that axis.\n",
        "\n",
        "\n",
        "## Matrices\n",
        "\n",
        "Just as vectors generalize scalars from order zero to order one,\n",
        "matrices generalize vectors from order one to order two.\n",
        "Matrices, which we will typically denote with bold-faced, capital letters\n",
        "(e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$),\n",
        "are represented in code as tensors with two axes.\n",
        "\n",
        "In math notation, we use $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
        "to express that the matrix $\\mathbf{A}$ consists of $m$ rows and $n$ columns of real-valued scalars.\n",
        "Visually, we can illustrate any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ as a table,\n",
        "where each element $a_{ij}$ belongs to the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n",
        "\n",
        "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
        "\n",
        "\n",
        "For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the shape of $\\mathbf{A}$\n",
        "is ($m$, $n$) or $m \\times n$.\n",
        "Specifically, when a matrix has the same number of rows and columns,\n",
        "its shape becomes a square; thus, it is called a *square matrix*.\n",
        "\n",
        "We can **create an $m \\times n$ matrix**\n",
        "by specifying a shape with two components $m$ and $n$\n",
        "when calling any of our favorite functions for instantiating a tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uCaonFpwerX",
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98e536e-f4c2-4625-861c-ad3370068cbf"
      },
      "source": [
        "# Nous créons une matrice 5 x 4 (donc avec 20 éléments)\n",
        "A = torch.arange(20).reshape(5, 4)\n",
        "A"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3],\n",
              "        [ 4,  5,  6,  7],\n",
              "        [ 8,  9, 10, 11],\n",
              "        [12, 13, 14, 15],\n",
              "        [16, 17, 18, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGQKb2CpM6BZ",
        "outputId": "f009df42-fcc7-427f-a3a6-6dc5cdbd94df"
      },
      "source": [
        "# Question: quel est la dimension de A ?\n",
        "A.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSe0FVpDwerY",
        "origin_pos": 24
      },
      "source": [
        "We can access the scalar element $a_{ij}$ of a matrix $\\mathbf{A}$ by specifying the indices for the row ($i$) and column ($j$),\n",
        "such as $[\\mathbf{A}]_{ij}$.\n",
        "\n",
        "Sometimes, we want to flip the axes.\n",
        "When we exchange a matrix's rows and columns,\n",
        "the result is called the *transpose* of the matrix.\n",
        "Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$\n",
        "and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for any $i$ and $j$.\n",
        "Thus, the transpose of $\\mathbf{A}$ in is\n",
        "a $n \\times m$ matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^\\top =\n",
        "\\begin{bmatrix}\n",
        "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
        "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
        "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
        "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Now we access a **matrix's transpose** in code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1NN6W1bwerY",
        "origin_pos": 26,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a960ee1-f538-4ae6-a306-aa04862fc2a8"
      },
      "source": [
        "# On affiche la matrice transposé de A\n",
        "A.T"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  4,  8, 12, 16],\n",
              "        [ 1,  5,  9, 13, 17],\n",
              "        [ 2,  6, 10, 14, 18],\n",
              "        [ 3,  7, 11, 15, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZevJEfc7NPFz"
      },
      "source": [
        "# Question: quel est le shape de A ?"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si0lRBexwerY",
        "origin_pos": 28
      },
      "source": [
        "As a special type of the square matrix,\n",
        "**a *symmetric matrix* $\\mathbf{A}$ is equal to its transpose:\n",
        "$\\mathbf{A} = \\mathbf{A}^\\top$.**\n",
        "Here we define a symmetric matrix `B`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4prm-03swerY",
        "origin_pos": 30,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2640fb-37ff-4deb-f391-238835b46ed7"
      },
      "source": [
        "# Autre façon de définir une matrice.\n",
        "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
        "B"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [2, 0, 4],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKDIjpwwerY",
        "origin_pos": 32
      },
      "source": [
        "Now we compare `B` with its transpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC5e9JMHwerY",
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a137a395-ccc0-4bed-aa78-15d683af740f"
      },
      "source": [
        "# On demande si les éléments de B sont égaux à B.T\n",
        "B == B.T"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW7gv57uBvl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dca95d6-54fe-4ffc-b58e-1424a0140259"
      },
      "source": [
        "C = torch.tensor([[1, 2, 2], [2, -1, 1], [-2, 0, 0]])\n",
        "C"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  2],\n",
              "        [ 2, -1,  1],\n",
              "        [-2,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5sLOny0Bvl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe6faf7-cd51-4f9e-9884-172291155007"
      },
      "source": [
        "# On peut vérifier si C est symétrique : \n",
        "C == C.T"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True, False],\n",
              "        [ True,  True, False],\n",
              "        [False, False,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL-Mhx9sOymG"
      },
      "source": [
        "# Question: Qu'est-ce que la transposée de la transposée ?\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RGUPnyPwerZ",
        "origin_pos": 36
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes.\n",
        "**Tensors**\n",
        "\"tensors\" in this subsection refer to algebraic objects\n",
        "**give us a generic way of describing $n$-dimensional arrays with an arbitrary number of axes.**\n",
        "Vectors, for example, are first-order tensors, and **matrices are second-order tensors.**\n",
        "Tensors are denoted with capital letters of a special font face\n",
        "(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)\n",
        "and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) is similar to that of matrices.\n",
        "\n",
        "Tensors will become more important when we start working with images,\n",
        " which arrive as $n$-dimensional arrays with 3 axes corresponding to the height, width, and a *channel* axis for stacking the color channels (red, green, and blue). For now, we will skip over higher order tensors and focus on the basics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAEmKqaawerZ",
        "origin_pos": 38,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89228e12-d496-4c06-d7de-6f7a115b6de4"
      },
      "source": [
        "# Tensor 3-dimmensionel\n",
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "X"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqwC7fHUBvl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2c2c7b-ca87-43fc-884c-592f8928efa2"
      },
      "source": [
        "# Pas de limite\n",
        "Y = torch.arange(120).reshape(2,3,4,5)\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[  0,   1,   2,   3,   4],\n",
              "          [  5,   6,   7,   8,   9],\n",
              "          [ 10,  11,  12,  13,  14],\n",
              "          [ 15,  16,  17,  18,  19]],\n",
              "\n",
              "         [[ 20,  21,  22,  23,  24],\n",
              "          [ 25,  26,  27,  28,  29],\n",
              "          [ 30,  31,  32,  33,  34],\n",
              "          [ 35,  36,  37,  38,  39]],\n",
              "\n",
              "         [[ 40,  41,  42,  43,  44],\n",
              "          [ 45,  46,  47,  48,  49],\n",
              "          [ 50,  51,  52,  53,  54],\n",
              "          [ 55,  56,  57,  58,  59]]],\n",
              "\n",
              "\n",
              "        [[[ 60,  61,  62,  63,  64],\n",
              "          [ 65,  66,  67,  68,  69],\n",
              "          [ 70,  71,  72,  73,  74],\n",
              "          [ 75,  76,  77,  78,  79]],\n",
              "\n",
              "         [[ 80,  81,  82,  83,  84],\n",
              "          [ 85,  86,  87,  88,  89],\n",
              "          [ 90,  91,  92,  93,  94],\n",
              "          [ 95,  96,  97,  98,  99]],\n",
              "\n",
              "         [[100, 101, 102, 103, 104],\n",
              "          [105, 106, 107, 108, 109],\n",
              "          [110, 111, 112, 113, 114],\n",
              "          [115, 116, 117, 118, 119]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUf2hIsywerZ",
        "origin_pos": 40
      },
      "source": [
        "## Basic Properties of Tensor Arithmetic\n",
        "\n",
        "Scalars, vectors, matrices, and tensors (\"tensors\" in this subsection refer to algebraic objects)\n",
        "of an arbitrary number of axes\n",
        "have some nice properties that often come in handy.\n",
        "For example, you might have noticed\n",
        "from the definition of an elementwise operation\n",
        "that any elementwise unary operation does not change the shape of its operand.\n",
        "Similarly,\n",
        "[**given any two tensors with the same shape,\n",
        "the result of any binary elementwise operation\n",
        "will be a tensor of that same shape.**]\n",
        "For example, adding two matrices of the same shape\n",
        "performs elementwise addition over these two matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mmqi5COwerZ",
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1ddc71-c435-41fd-a64b-33f041bf5530"
      },
      "source": [
        "# On crée deux matrices et on les affiche\n",
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  #B est une copie de A\n",
        "A, B"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]), tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DMbAGs8V5Y7",
        "outputId": "b67c3d8d-4644-4061-df03-c69c4db4eb64"
      },
      "source": [
        "A + B # On affiche la somme A+B"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  2.,  4.,  6.],\n",
              "        [ 8., 10., 12., 14.],\n",
              "        [16., 18., 20., 22.],\n",
              "        [24., 26., 28., 30.],\n",
              "        [32., 34., 36., 38.]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDL_W_M_Bvl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19dedf95-796b-4f0e-9b7d-a7e1aeea25c8"
      },
      "source": [
        "# Somme de matrices\n",
        "X = torch.tensor([[1,2],[-1,1]])\n",
        "Y = torch.tensor([[0,2],[-2,3]])\n",
        "X"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2],\n",
              "        [-1,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEmQkKaiUT0z",
        "outputId": "780e8814-f6f4-4608-9d09-8e010ac2c959"
      },
      "source": [
        "Y"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  2],\n",
              "        [-2,  3]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw8W6NuGBvl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0557d16b-2b3d-4fbc-c40a-d748fd96408b"
      },
      "source": [
        "X + Y"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  4],\n",
              "        [-3,  4]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ARpipz4UsSC",
        "outputId": "481d7ef0-cbb3-4b8c-8ec8-fed5a8d3a078"
      },
      "source": [
        "Z = torch.tensor([[1,2,1],[-1,1,0],[-1,1,3]])\n",
        "Z"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  1],\n",
              "        [-1,  1,  0],\n",
              "        [-1,  1,  3]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "oIkEnTzvU6ac",
        "outputId": "d4768d16-db01-47e3-d73a-94452fc35468"
      },
      "source": [
        "# Afficher X+Z\n",
        "X+Z "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-7f61030c60ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Afficher X+Z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebWndtObwerZ",
        "origin_pos": 44
      },
      "source": [
        "Specifically,\n",
        "**elementwise multiplication of two matrices is called their *Hadamard product***\n",
        "(math notation $\\odot$).\n",
        "Consider matrix $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ whose element of row $i$ and column $j$ is $b_{ij}$. The Hadamard product of matrices $\\mathbf{A}$ and $\\mathbf{B}$\n",
        "\n",
        "$$\n",
        "\\mathbf{A} \\odot \\mathbf{B} =\n",
        "\\begin{bmatrix}\n",
        "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
        "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
        "\\end{bmatrix}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_VzY1hGwerZ",
        "origin_pos": 46,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa926390-dafc-4d18-8138-f2c1b3ecd2cd"
      },
      "source": [
        "A * B # Attention, cela n'est pas le produit usuel de matrices"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.,   1.,   4.,   9.],\n",
              "        [ 16.,  25.,  36.,  49.],\n",
              "        [ 64.,  81., 100., 121.],\n",
              "        [144., 169., 196., 225.],\n",
              "        [256., 289., 324., 361.]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pRMUb5hwera",
        "origin_pos": 48
      },
      "source": [
        "**Multiplying or adding a tensor by a scalar** also does not change the shape of the tensor,\n",
        "where each element of the operand tensor will be added or multiplied by the scalar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnVkhxugwera",
        "origin_pos": 50,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e5b47b-b059-4993-ecf4-b6265e22fc49"
      },
      "source": [
        "a = 2 # la variable a est declarer égale au scalaire 2\n",
        "X = torch.arange(24).reshape(2, 3, 4) # On crée un tenseur 3 dimmentionel\n",
        "a + X"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2,  3,  4,  5],\n",
              "         [ 6,  7,  8,  9],\n",
              "         [10, 11, 12, 13]],\n",
              "\n",
              "        [[14, 15, 16, 17],\n",
              "         [18, 19, 20, 21],\n",
              "         [22, 23, 24, 25]]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIw_-TcNVITt",
        "outputId": "fde5b8d3-8804-4cb5-b01f-e9005d4b550a"
      },
      "source": [
        "# Le shape reste le meme\n",
        "(a * X).shape, X.shape"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6XFxX1Xwera",
        "origin_pos": 52
      },
      "source": [
        "## Reduction\n",
        "\n",
        "One useful operation that we can perform with arbitrary tensors\n",
        "is to\n",
        "calculate **the sum of their elements.**\n",
        "In mathematical notation, we express sums using the $\\sum$ symbol.\n",
        "To express the sum of the elements in a vector $\\mathbf{x}$ of length $d$,\n",
        "we write $\\sum_{i=1}^d x_i$.\n",
        "In code, we can just call the function for calculating the sum.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZthKwhpwera",
        "origin_pos": 54,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1f3b73-2d15-4916-c4b5-c6641bb63959"
      },
      "source": [
        "# On créé un vecteur et on affiche la somme de ses elements\n",
        "x = torch.arange(4, dtype=torch.float32)\n",
        "x, x.sum()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.]), tensor(6.))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXKyP1gxBvmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf5f628-3d36-47df-e01c-029171365f02"
      },
      "source": [
        "# Autre exemple \n",
        "y = torch.tensor([1,-2,3])\n",
        "y, y.sum()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1, -2,  3]), tensor(2))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGSlImKwVaHl",
        "outputId": "d1c93c79-21bf-4616-fc71-d42ac8c4b8a0"
      },
      "source": [
        "# Un commentaire sur les types de variables numeriques\n",
        "x.sum().dtype, y[2].dtype"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tG7Tu1-wera",
        "origin_pos": 56
      },
      "source": [
        "We can express **sums over the elements of tensors of arbitrary shape.**\n",
        "For example, the sum of the elements of an $m \\times n$ matrix $\\mathbf{A}$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM2BOyaBwera",
        "origin_pos": 58,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21ea918-2ffd-4bdf-cba4-b4178249dafe"
      },
      "source": [
        "A, A.shape, A.sum()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]), torch.Size([5, 4]), tensor(190.))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHF6MV9mwera",
        "origin_pos": 60
      },
      "source": [
        "By default, invoking the function for calculating the sum\n",
        "*reduces* a tensor along all its axes to a scalar.\n",
        "We can also **specify the axes along which the tensor is reduced via summation.**\n",
        "Take matrices as an example.\n",
        "To reduce the row dimension (axis 0) by summing up elements of all the rows,\n",
        "we specify `axis=0` when invoking the function.\n",
        "Since the input matrix reduces along axis 0 to generate the output vector,\n",
        "the dimension of axis 0 of the input is lost in the output shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6BQSvDgZBvmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62ff37b-35c8-4a18-cd04-a39110f7b88d"
      },
      "source": [
        "# Re-afficher A\n",
        "A"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.],\n",
              "        [12., 13., 14., 15.],\n",
              "        [16., 17., 18., 19.]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyY2VOYUwera",
        "origin_pos": 62,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bd8430-40f4-4a7e-b989-9dcfd8386adf"
      },
      "source": [
        "# Somme des elements de chaque colonne\n",
        "A_sum_axis0 = A.sum(axis=0)\n",
        "A_sum_axis0, A_sum_axis0.shape"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biUQvP4oBvmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f06a50c-ee4e-432f-c7b0-b58ad80eae62"
      },
      "source": [
        "# Somme des elements de la colonne 2\n",
        "A_sum_axis0[1]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(45.)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7vFyR0Awera",
        "origin_pos": 64
      },
      "source": [
        "Specifying\n",
        "`axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns.\n",
        "Thus, the dimension of axis 1 of the input is lost in the output shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHWXk6jGwera",
        "origin_pos": 66,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6643e49-8213-4de3-e2f1-4bd53c8a060c"
      },
      "source": [
        "# Somme des elements de chaque ligne\n",
        "A_sum_axis1 = A.sum(axis=1)\n",
        "A_sum_axis1, A_sum_axis1.shape"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU-MbK9Awera",
        "origin_pos": 68
      },
      "source": [
        "Reducing a matrix along both rows and columns via summation\n",
        "is equivalent to summing up all the elements of the matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF4K6sE6wera",
        "origin_pos": 70,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13baa80b-e825-4d88-a205-2785daf3e677"
      },
      "source": [
        "A.sum(axis=[0, 1])  # La même choque que 'A.sum()''"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(190.)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC69xqhEwera",
        "origin_pos": 72
      },
      "source": [
        "**A related quantity is the *mean*, which is also called the *average*.**\n",
        "\n",
        "We calculate the mean by dividing the sum by the total number of elements.\n",
        "In code, we could just call the function for calculating the mean\n",
        "on tensors of arbitrary shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkVhW4EsWAet",
        "outputId": "ee3f2fe4-d016-4e1f-a7b5-78bc2b9b5c79"
      },
      "source": [
        "# Quantité d'elements de la matrice A\n",
        "A.numel()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn72CNIrwera",
        "origin_pos": 74,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdccac42-1083-4faa-d83f-f28fa4214021"
      },
      "source": [
        "# Moyenne : somme divisée par la quantité d'elements\n",
        "A.mean(), A.sum() / A.numel()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(9.5000), tensor(9.5000))"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu9R3GMywerb",
        "origin_pos": 92
      },
      "source": [
        "## Dot Products\n",
        "\n",
        "So far, we have only performed elementwise operations, sums, and averages. And if this was all we could do, linear algebra probably would not deserve its own section. However, one of the most fundamental operations is the dot product.\n",
        "Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their *dot product* $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$) is a sum over the products of the elements at the same position: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$.\n",
        "\n",
        "[~~The *dot product* of two vectors is a sum over the products of the elements at the same position~~]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL_theVtwerb",
        "origin_pos": 94,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0400e649-f27c-44a5-8f9a-9c13db2850ca"
      },
      "source": [
        "# Un vecteur de dimmension 4 avec des 1\n",
        "y = torch.ones(4, dtype=torch.float32)\n",
        "y"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwhwBSb_WZ4o",
        "outputId": "490423c5-4238-4829-c825-ca6082eeabaf"
      },
      "source": [
        "x"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U5uQTaVWdIw",
        "outputId": "932d1478-20d4-4b7a-c814-70afb8b62c4a"
      },
      "source": [
        "# le produit scalaire de x et y\n",
        "torch.dot(x, y)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk3dM61nBvmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94648b35-4678-42c4-c9a7-e0d9c1b2e83a"
      },
      "source": [
        "# Autre exemple\n",
        "u = torch.tensor([-1,1,3])\n",
        "v = torch.tensor([1,0,2])\n",
        "torch.dot(u, v)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E420TNM9werb",
        "origin_pos": 96
      },
      "source": [
        "Note that\n",
        "**we can express the dot product of two vectors equivalently by performing an elementwise multiplication and then a sum:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL-uSrVywerb",
        "origin_pos": 98,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ca8618-0b36-46a5-9250-e5207de2b2bc"
      },
      "source": [
        "torch.sum(x * y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XONPpTnwwerb",
        "origin_pos": 100
      },
      "source": [
        "Dot products are useful in a wide range of contexts.\n",
        "For example, given some set of values,\n",
        "denoted by a vector $\\mathbf{x}  \\in \\mathbb{R}^d$\n",
        "and a set of weights denoted by $\\mathbf{w} \\in \\mathbb{R}^d$,\n",
        "the weighted sum of the values in $\\mathbf{x}$\n",
        "according to the weights $\\mathbf{w}$\n",
        "could be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}$.\n",
        "When the weights are non-negative\n",
        "and sum to one (i.e., $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$),\n",
        "the dot product expresses a *weighted average*.\n",
        "After normalizing two vectors to have the unit length,\n",
        "the dot products express the cosine of the angle between them.\n",
        "We will formally introduce this notion of *length* later in this section.\n",
        "\n",
        "\n",
        "## Matrix-Vector Products\n",
        "\n",
        "Now that we know how to calculate dot products,\n",
        "we can begin to understand *matrix-vector products*.\n",
        "Recall the matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
        "and the vector $\\mathbf{x} \\in \\mathbb{R}^n$\n",
        "defined and visualized in :eqref:`eq_matrix_def` and :eqref:`eq_vec_def` respectively.\n",
        "Let us start off by visualizing the matrix $\\mathbf{A}$ in terms of its row vectors\n",
        "\n",
        "$$\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_m \\\\\n",
        "\\end{bmatrix},$$\n",
        "\n",
        "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$\n",
        "is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$.\n",
        "\n",
        "[**The matrix-vector product $\\mathbf{A}\\mathbf{x}$\n",
        "is simply a column vector of length $m$,\n",
        "whose $i^\\mathrm{th}$ element is the dot product $\\mathbf{a}^\\top_i \\mathbf{x}$:**]\n",
        "\n",
        "$$\n",
        "\\mathbf{A}\\mathbf{x}\n",
        "= \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_m \\\\\n",
        "\\end{bmatrix}\\mathbf{x}\n",
        "= \\begin{bmatrix}\n",
        " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
        " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
        "\\vdots\\\\\n",
        " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "We can think of multiplication by a matrix $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$\n",
        "as a transformation that projects vectors\n",
        "from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n",
        "These transformations turn out to be remarkably useful.\n",
        "For example, we can represent rotations\n",
        "as multiplications by a square matrix.\n",
        "As we will see in subsequent chapters,\n",
        "we can also use matrix-vector products\n",
        "to describe the most intensive calculations\n",
        "required when computing each layer in a neural network\n",
        "given the values of the previous layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJhqEa8Mwerc",
        "origin_pos": 102,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "Expressing matrix-vector products in code with tensors, we use\n",
        "the `mv` function. When we call `torch.mv(A, x)` with a matrix\n",
        "`A` and a vector `x`, the matrix-vector product is performed.\n",
        "Note that the column dimension of `A` (its length along axis 1)\n",
        "must be the same as the dimension of `x` (its length).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffAaMMscBvmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f96b9f0-3a7f-4971-9b32-e71b0fe85a90"
      },
      "source": [
        "# On affiche A et x\n",
        "A,x"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]), tensor([0., 1., 2., 3.]))"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8XLwnqQwerc",
        "origin_pos": 105,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984fb275-db43-41f0-97bc-a2b0c659d132"
      },
      "source": [
        "A.shape, x.shape, torch.mv(A, x)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpOcyi9vwerc",
        "origin_pos": 107
      },
      "source": [
        "## Matrix-Matrix Multiplication\n",
        "\n",
        "If you have gotten the hang of dot products and matrix-vector products,\n",
        "then *matrix-matrix multiplication* should be straightforward.\n",
        "\n",
        "Say that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
        "\n",
        "$$\\mathbf{A}=\\begin{bmatrix}\n",
        " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
        " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
        "\\end{bmatrix},\\quad\n",
        "\\mathbf{B}=\\begin{bmatrix}\n",
        " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
        " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "\n",
        "Denote by $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$\n",
        "the row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$,\n",
        "and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$\n",
        "be the column vector from the $j^\\mathrm{th}$ column of the matrix $\\mathbf{B}$.\n",
        "To produce the matrix product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, it is easiest to think of $\\mathbf{A}$ in terms of its row vectors and $\\mathbf{B}$ in terms of its column vectors:\n",
        "\n",
        "$$\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_n \\\\\n",
        "\\end{bmatrix},\n",
        "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
        " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "Then the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ is produced as we simply compute each element $c_{ij}$ as the dot product $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
        "\n",
        "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_n \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
        " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
        " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
        "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "[**We can think of the matrix-matrix multiplication $\\mathbf{AB}$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix.**]\n",
        "In the following snippet, we perform matrix multiplication on `A` and `B`.\n",
        "Here, `A` is a matrix with 5 rows and 4 columns,\n",
        "and `B` is a matrix with 4 rows and 3 columns.\n",
        "After multiplication, we obtain a matrix with 5 rows and 3 columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRyFEt3gW_Qz",
        "outputId": "961481ad-f1e7-4771-dab8-5969d7c3aefb"
      },
      "source": [
        "# On re-affiche A\n",
        "A"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.],\n",
              "        [12., 13., 14., 15.],\n",
              "        [16., 17., 18., 19.]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPXvjAe8werc",
        "origin_pos": 109,
        "scrolled": true,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afd0266-4145-4610-dcad-951c98f5289b"
      },
      "source": [
        "# On crée une matrice avec des 1\n",
        "B = torch.ones(4, 3)\n",
        "B"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmHaEW-sBvmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b1b55d-8cdc-47f9-99d5-446aa58fc158"
      },
      "source": [
        "# La fonction mm donne le produit de A et B\n",
        "torch.mm(A, B)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.,  6.,  6.],\n",
              "        [22., 22., 22.],\n",
              "        [38., 38., 38.],\n",
              "        [54., 54., 54.],\n",
              "        [70., 70., 70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaBHymZYwerd",
        "origin_pos": 111
      },
      "source": [
        "Matrix-matrix multiplication can be simply called **matrix multiplication**, and **should not be confused with the Hadamard product**.\n",
        "\n",
        "\n",
        "## Norms\n",
        "\n",
        "Some of the most useful operators in linear algebra are *norms*.\n",
        "Informally, the norm of a vector tells us how *big* a vector is.\n",
        "The notion of *size* under consideration here\n",
        "concerns not dimensionality\n",
        "but rather the magnitude of the components.\n",
        "\n",
        "In linear algebra, a vector norm is a function $f$ that maps a vector\n",
        "to a scalar, satisfying a handful of properties.\n",
        "Given any vector $\\mathbf{x}$,\n",
        "the first property says\n",
        "that if we scale all the elements of a vector\n",
        "by a constant factor $\\alpha$,\n",
        "its norm also scales by the *absolute value*\n",
        "of the same constant factor:\n",
        "\n",
        "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
        "\n",
        "\n",
        "The second property is the familiar triangle inequality:\n",
        "\n",
        "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
        "\n",
        "\n",
        "The third property simply says that the norm must be non-negative:\n",
        "\n",
        "$$f(\\mathbf{x}) \\geq 0.$$\n",
        "\n",
        "That makes sense, as in most contexts the smallest *size* for anything is 0.\n",
        "The final property requires that the smallest norm is achieved and only achieved\n",
        "by a vector consisting of all zeros.\n",
        "\n",
        "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
        "\n",
        "You might notice that norms sound a lot like measures of distance.\n",
        "And if you remember Euclidean distances\n",
        "(think Pythagoras' theorem) from grade school,\n",
        "then the concepts of non-negativity and the triangle inequality might ring a bell.\n",
        "In fact, the Euclidean distance is a norm:\n",
        "specifically it is the $L_2$ norm.\n",
        "Suppose that the elements in the $n$-dimensional vector\n",
        "$\\mathbf{x}$ are $x_1, \\ldots, x_n$.\n",
        "\n",
        "[**The $L_2$ *norm* of $\\mathbf{x}$ is the square root of the sum of the squares of the vector elements:**]\n",
        "\n",
        "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n",
        "\n",
        "\n",
        "where the subscript $2$ is often omitted in $L_2$ norms, i.e., $\\|\\mathbf{x}\\|$ is equivalent to $\\|\\mathbf{x}\\|_2$. In code,\n",
        "we can calculate the $L_2$ norm of a vector as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z73BARBwerd",
        "origin_pos": 113,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f16b508-c8b0-4240-9815-ed5ac31d6ea3"
      },
      "source": [
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33WkXnkJwerd",
        "origin_pos": 115
      },
      "source": [
        "In deep learning, we work more often\n",
        "with the squared $L_2$ norm.\n",
        "\n",
        "You will also frequently encounter [**the $L_1$ *norm***],\n",
        "which is expressed as the sum of the absolute values of the vector elements:\n",
        "\n",
        "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
        "\n",
        "\n",
        "As compared with the $L_2$ norm,\n",
        "it is less influenced by outliers.\n",
        "To calculate the $L_1$ norm, we compose\n",
        "the absolute value function with a sum over the elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feBwe7ZWwerd",
        "origin_pos": 117,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09acfff-c612-4d2b-e4d9-274b8b424626"
      },
      "source": [
        "torch.abs(u).sum()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnHzyXbdwerd",
        "origin_pos": 119
      },
      "source": [
        "Both the $L_2$ norm and the $L_1$ norm\n",
        "are special cases of the more general $L_p$ *norm*:\n",
        "\n",
        "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
        "\n",
        "Analogous to $L_2$ norms of vectors,\n",
        "[**the *Frobenius norm* of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$**]\n",
        "is the square root of the sum of the squares of the matrix elements:\n",
        "\n",
        "[**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**]\n",
        "\n",
        "The Frobenius norm satisfies all the properties of vector norms.\n",
        "It behaves as if it were an $L_2$ norm of a matrix-shaped vector.\n",
        "Invoking the following function will calculate the Frobenius norm of a matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjy8sC9DBvmE",
        "outputId": "fa64441f-6a23-4330-91b5-3013621fe642"
      },
      "source": [
        "torch.ones((4, 9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAPPlPBrwerd",
        "origin_pos": 121,
        "tab": [
          "pytorch"
        ],
        "outputId": "b0ca4bcc-4ef1-4783-ef14-0e6e5a04211d"
      },
      "source": [
        "torch.norm(torch.ones((4, 9)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khlKrsd_werd",
        "origin_pos": 123
      },
      "source": [
        "## Norms and Objectives\n",
        "\n",
        "While we do not want to get too far ahead of ourselves,\n",
        "we can plant some intuition already about why these concepts are useful.\n",
        "In deep learning, we are often trying to solve optimization problems:\n",
        "*maximize* the probability assigned to observed data;\n",
        "*minimize* the distance between predictions\n",
        "and the ground-truth observations.\n",
        "Assign vector representations to items (like words, products, or news articles)\n",
        "such that the distance between similar items is minimized,\n",
        "and the distance between dissimilar items is maximized.\n",
        "Oftentimes, the objectives, perhaps the most important components\n",
        "of deep learning algorithms (besides the data),\n",
        "are expressed as norms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrkF4nXWYLvu"
      },
      "source": [
        "**Merci pour votre attention !**\n",
        "\n",
        "Des questions ?!"
      ]
    }
  ]
}